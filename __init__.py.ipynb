{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import attr\n",
    "import urllib\n",
    "import requests\n",
    "import backoff\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import datetime\n",
    "import dateutil\n",
    "from dateutil import parser\n",
    "import singer\n",
    "import singer.metrics as metrics\n",
    "from singer import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_CONFIG_KEYS = [\"url\",\"datasetid\", \"api_key\", \"start_date\", \"end_date\"]\n",
    "LOGGER = singer.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINTS = {\n",
    "    \"gsom\":\"datasetid=GSOM&startdate={0}&enddate={1}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = json.loads(open('config.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_endpoint(endpoint, kwargs):\n",
    "    '''Get the full url for the endpoint'''\n",
    "    if endpoint not in ENDPOINTS:\n",
    "        raise ValueError(\"Invalid endpoint {}\".format(endpoint))\n",
    "    \n",
    "    datasetid = urllib.parse.quote(kwargs[0])\n",
    "    startdate = kwargs[1]\n",
    "    enddate = kwargs[2]\n",
    "    return CONFIG[\"url\"]+ENDPOINTS[endpoint].format(datasetid,startdate,enddate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start(STATE, tap_stream_id, bookmark_key):\n",
    "    current_bookmark = singer.get_bookmark(STATE, tap_stream_id, bookmark_key)\n",
    "    if current_bookmark is None:\n",
    "        return CONFIG[\"start_date\"]\n",
    "    return current_bookmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_schema(entity):\n",
    "    '''Returns the schema for the specified source'''\n",
    "    schema = utils.load_json(get_abs_path(\"schemas/{}.json\".format(entity)))\n",
    "\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dates(result):\n",
    "    tzinfo = parser.parse(CONFIG[\"start_date\"]).tzinfo\n",
    "    filtered = {\n",
    "        \"date\":parser.parse(result[\"date\"]).replace(tzinfo=tzinfo).isoformat(),\n",
    "        \"datatype\":result[\"datatype\"],\n",
    "        \"station\":,result[\"station\"],\n",
    "        \"attributes\":result[\"attributes\"],\n",
    "        \"value\": result[\"value\"]\n",
    "    }\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def giveup(exc):\n",
    "    return exc.response is not None \\\n",
    "        and 400 <= exc.response.status_code < 500 \\\n",
    "        and exc.response.status_code != 429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.backoff((backoff.expo,requests.exceptions.RequestException), giveup)\n",
    "@utils.ratelimit(20, 1)\n",
    "def gen_request(stream_id, url):\n",
    "    with metrics.http_request_timer(stream_id) as timer:\n",
    "        resp = requests.get(url, headers={\"token\":CONFIG[api_key]})\n",
    "        timer.tags[metrics.Tag.http_status_code] = resp.status_code\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_gsom(STATE, catalog):\n",
    "    schema = load_schema(\"gsom\")\n",
    "    singer.write_schema(\"gsom\", schema, [\"order_id\"])\n",
    "\n",
    "    start = get_start(STATE, \"gsom\", \"last_update\")\n",
    "    LOGGER.info(\"Only syncing gsom updated since \" + start)\n",
    "    last_update = start\n",
    "    page_number = 1\n",
    "    with metrics.record_counter(\"gsom\") as counter:\n",
    "        while True:\n",
    "            endpoint = get_endpoint(\"gsom\", [start])\n",
    "            LOGGER.info(\"GET %s\", endpoint)\n",
    "            response = gen_request(\"gsom\",endpoint)\n",
    "            for result in response.results:\n",
    "                counter.increment()\n",
    "#                 result = filter_dates(result)\n",
    "#                 if(\"date\" in result) and (parser.parse(result[\"date\"]) > parser.parse(last_update)):\n",
    "#                     last_update = result[\"date\"]\n",
    "                singer.write_record(\"gsom\", result)\n",
    "            if len(orders) < 100:\n",
    "                break\n",
    "            else:\n",
    "                page_number +=1\n",
    "    STATE = singer.write_bookmark(STATE, 'gsom', 'last_update', last_update) \n",
    "    singer.write_state(STATE)\n",
    "    LOGGER.info(\"Completed Orders Sync\")\n",
    "    return STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class Stream(object):\n",
    "    tap_stream_id = attr.ib()\n",
    "    sync = attr.ib()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "STREAMS = [\n",
    "    Stream(\"gsom\", sync_gsom)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_streams_to_sync(streams, state):\n",
    "    '''Get the streams to sync'''\n",
    "    current_stream = singer.get_currently_syncing(state)\n",
    "    result = streams\n",
    "    if current_stream:\n",
    "        result = list(itertools.dropwhile(\n",
    "            lambda x: x.tap_stream_id != current_stream, streams))\n",
    "    if not result:\n",
    "        raise Exception(\"Unknown stream {} in state\".format(current_stream))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_streams(remaining_streams, annotated_schema):\n",
    "    selected_streams = []\n",
    "\n",
    "    for stream in remaining_streams:\n",
    "        tap_stream_id = stream.tap_stream_id\n",
    "        for stream_idx, annotated_stream in enumerate(annotated_schema.streams):\n",
    "            if tap_stream_id == annotated_stream.tap_stream_id:\n",
    "                schema = annotated_stream.schema\n",
    "                if (hasattr(schema, \"selected\")) and (schema.selected is True):\n",
    "                    selected_streams.append(stream)\n",
    "\n",
    "    return selected_streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_sync(STATE, catalogs):\n",
    "    '''Sync the streams that were selected'''\n",
    "    remaining_streams = get_streams_to_sync(STREAMS, STATE)\n",
    "    selected_streams = get_selected_streams(remaining_streams, catalogs)\n",
    "    if len(selected_streams) < 1:\n",
    "        LOGGER.info(\"No Streams selected, please check that you have a schema selected in your catalog\")\n",
    "        return\n",
    "\n",
    "    LOGGER.info(\"Starting sync. Will sync these streams: %s\", [stream.tap_stream_id for stream in selected_streams])\n",
    "\n",
    "    for stream in selected_streams:\n",
    "        LOGGER.info(\"Syncing %s\", stream.tap_stream_id)\n",
    "        singer.set_currently_syncing(STATE, stream.tap_stream_id)\n",
    "        singer.write_state(STATE)\n",
    "\n",
    "        try:\n",
    "            catalog = [cat for cat in catalogs.streams if cat.stream == stream.tap_stream_id][0]\n",
    "            STATE = stream.sync(STATE, catalog)\n",
    "        except Exception as e:\n",
    "            LOGGER.critical(e)\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abs_path(path):\n",
    "    '''Returns the absolute path'''\n",
    "    return os.path.join(os.path.dirname(os.path.realpath(__file__)), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_discovered_schema(stream):\n",
    "    '''Attach inclusion automatic to each schema'''\n",
    "    schema = load_schema(stream.tap_stream_id)\n",
    "    for k in schema['properties']:\n",
    "        schema['properties'][k]['inclusion'] = 'automatic'\n",
    "    return schema\n",
    "\n",
    "def discover_schemas():\n",
    "    '''Iterate through streams, push to an array and return'''\n",
    "    result = {'streams': []}\n",
    "    for stream in STREAMS:\n",
    "        LOGGER.info('Loading schema for %s', stream.tap_stream_id)\n",
    "        result['streams'].append({'stream': stream.tap_stream_id,\n",
    "                                  'tap_stream_id': stream.tap_stream_id,\n",
    "                                  'schema': load_discovered_schema(stream)})\n",
    "    return result\n",
    "\n",
    "def do_discover():\n",
    "    '''JSON dump the schemas to stdout'''\n",
    "    LOGGER.info(\"Loading Schemas\")\n",
    "    json.dump(discover_schemas(), sys.stdout, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.handle_top_exception(LOGGER)\n",
    "def main():\n",
    "    '''Entry point'''\n",
    "    args = utils.parse_args(REQUIRED_CONFIG_KEYS)\n",
    "\n",
    "    CONFIG.update(args.config)\n",
    "    STATE = {}\n",
    "\n",
    "    if args.state:\n",
    "        STATE.update(args.state)\n",
    "    if args.discover:\n",
    "        do_discover()\n",
    "    elif args.properties:\n",
    "        do_sync(STATE, args.properties)\n",
    "    elif args.catalog:\n",
    "        do_sync(STATE, args.catalog)\n",
    "    else:\n",
    "        LOGGER.info(\"No Streams were selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
